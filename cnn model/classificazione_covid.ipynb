{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:01:08.665026Z",
     "start_time": "2024-07-10T17:01:08.660230Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 16:58:28.538125: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-11 16:58:28.878675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-11 16:58:28.878705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-11 16:58:28.917634: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-11 16:58:28.998442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11 16:58:30.120241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from complexcgr import FCGR\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:01:08.710828Z",
     "start_time": "2024-07-10T17:01:08.693302Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Percorso delle immagini npy\n",
    "# input_path = r'C:\\Users\\GENNY C\\Desktop\\Bioinformatica_covid\\output\\7mer\\cladeO'\n",
    "# output_path = os.path.join(input_path, 'img')\n",
    "\n",
    "# # Creare la cartella di output\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# # Inizializzare l'oggetto FCGR\n",
    "# fcgr = FCGR(k=7)\n",
    "\n",
    "# # Funzione per caricare e salvare l'immagine\n",
    "# def load_and_save_image(npy_file, output_folder):\n",
    "#     # Caricare il file npy\n",
    "#     chaos = np.load(npy_file)\n",
    "    \n",
    "#     #Ottenere il nome del file senza estensione\n",
    "#     file_name = os.path.splitext(os.path.basename(npy_file))[0]\n",
    "    \n",
    "#     # Creare il percorso per salvare l'immagine\n",
    "#     jpeg_file = os.path.join(output_folder, f'{file_name}_O.jpg')\n",
    "    \n",
    "#     #Salvare l'immagine\n",
    "#     fcgr.save_img(chaos, jpeg_file)\n",
    "\n",
    "# # Caricare e convertire ogni file\n",
    "# for npy_file in glob.glob(os.path.join(input_path, '*.npy')):\n",
    "#     load_and_save_image(npy_file, output_path)\n",
    "\n",
    "# print(\"Conversione completata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:01:08.726363Z",
     "start_time": "2024-07-10T17:01:08.710828Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Percorso delle immagini npy\n",
    "# input_path = r'C:\\Users\\GENNY C\\Desktop\\Bioinformatica_covid\\output\\7mer\\cladeS'\n",
    "# output_path = os.path.join(input_path, 'img')\n",
    "\n",
    "# # Creare la cartella di output se non esiste\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# # Inizializzare l'oggetto FCGR\n",
    "# fcgr = FCGR(k=7)\n",
    "\n",
    "# # Funzione per caricare e salvare l'immagine\n",
    "# def load_and_save_image(npy_file, output_folder):\n",
    "    \n",
    "#     chaos = np.load(npy_file)\n",
    "    \n",
    "#     #Ottenere il nome del file senza estensione\n",
    "#     file_name = os.path.splitext(os.path.basename(npy_file))[0]\n",
    "    \n",
    "#     #Creare il percorso per salvare l'immagine\n",
    "#     jpeg_file = os.path.join(output_folder, f'{file_name}_S.jpg')\n",
    "    \n",
    "#     # Salvare l'immagine\n",
    "#     fcgr.save_img(chaos, jpeg_file)\n",
    "\n",
    "# #Caricare e convertire ogni file\n",
    "# for npy_file in glob.glob(os.path.join(input_path, '*.npy')):\n",
    "#     load_and_save_image(npy_file, output_path)\n",
    "\n",
    "# print(\"Conversione completata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:01:08.741718Z",
     "start_time": "2024-07-10T17:01:08.727361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CSV creato: /media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/GK/images_list_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "#Percorso delle immagini jpg\n",
    "#image_path = r'C:\\Users\\GENNY C\\Desktop\\Bioinformatica_covid\\output\\7mer\\cladeO\\img'\n",
    "image_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/GK/img'\n",
    "output_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/GK'\n",
    "csv_file_path = os.path.join(output_path, 'images_list_with_labels.csv')\n",
    "\n",
    "# Ottenere tutti i file jpg nella cartella\n",
    "image_files = glob.glob(os.path.join(image_path, '*.jpeg'))\n",
    "\n",
    "# Creare una lista con i nomi dei file e la label 0\n",
    "data = [{'Image_Name': os.path.basename(image_file), 'Label': 0} for image_file in image_files]\n",
    "\n",
    "# Creare un DataFrame con i nomi delle immagini e delle label\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Salvare il DataFrame come CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"File CSV creato: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hCoV-19_Denmark_DCGC-650232_2021_EPI_ISL_18150...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hCoV-19_Italy_SIC-AOUME-UOSD-GCL-209425482-356...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hCoV-19_Bangladesh_ideSHi-0531119_2021_EPI_ISL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hCoV-19_Bangladesh_ideSHi-063035_2021_EPI_ISL_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hCoV-19_Bangladesh_ideSHi-072_2021_EPI_ISL_188...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Image_Name  Label\n",
       "0  hCoV-19_Denmark_DCGC-650232_2021_EPI_ISL_18150...      0\n",
       "1  hCoV-19_Italy_SIC-AOUME-UOSD-GCL-209425482-356...      0\n",
       "2  hCoV-19_Bangladesh_ideSHi-0531119_2021_EPI_ISL...      0\n",
       "3  hCoV-19_Bangladesh_ideSHi-063035_2021_EPI_ISL_...      0\n",
       "4  hCoV-19_Bangladesh_ideSHi-072_2021_EPI_ISL_188...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CSV creato: /media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/V/images_list_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "#Percorso delle immagini jpg\n",
    "#image_path = r'C:\\Users\\GENNY C\\Desktop\\Bioinformatica_covid\\output\\7mer\\cladeO\\img'\n",
    "image_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/V/img'\n",
    "output_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/V'\n",
    "csv_file_path = os.path.join(output_path, 'images_list_with_labels.csv')\n",
    "\n",
    "# Ottenere tutti i file jpg nella cartella\n",
    "image_files = glob.glob(os.path.join(image_path, '*.jpeg'))\n",
    "\n",
    "# Creare una lista con i nomi dei file e la label 1\n",
    "data = [{'Image_Name': os.path.basename(image_file), 'Label': 1} for image_file in image_files]\n",
    "\n",
    "# Creare un DataFrame con i nomi delle immagini e delle label\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Salvare il DataFrame come CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"File CSV creato: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hCoV-19_England_SHEF-C00EE_2020_EPI_ISL_420274...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hCoV-19_Anhui_FY002_2020_EPI_ISL_424352_2020-0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hCoV-19_Argentina_PAIS-A0171_2020_EPI_ISL_7922...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hCoV-19_Argentina_PAIS-A0222_2020_EPI_ISL_7923...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hCoV-19_Argentina_PAIS-A0423_2020_EPI_ISL_1396...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Image_Name  Label\n",
       "0  hCoV-19_England_SHEF-C00EE_2020_EPI_ISL_420274...      1\n",
       "1  hCoV-19_Anhui_FY002_2020_EPI_ISL_424352_2020-0...      1\n",
       "2  hCoV-19_Argentina_PAIS-A0171_2020_EPI_ISL_7922...      1\n",
       "3  hCoV-19_Argentina_PAIS-A0222_2020_EPI_ISL_7923...      1\n",
       "4  hCoV-19_Argentina_PAIS-A0423_2020_EPI_ISL_1396...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:01:08.818985Z",
     "start_time": "2024-07-10T17:01:08.757876Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_file_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/GK/images_list_with_labels.csv'\n",
    "csv_file_path1 = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/V/images_list_with_labels.csv'\n",
    "\n",
    "df1 = pd.read_csv(csv_file_path)\n",
    "df2 = pd.read_csv(csv_file_path1)\n",
    "\n",
    "df_merge = pd.concat([df1, df2])\n",
    "output_csv_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer'\n",
    "output_csv_path = os.path.join(output_csv_path, 'df_completo_GKV_13kmer.csv')\n",
    "\n",
    "# Salvare il DataFrame unito come un nuovo file CSV\n",
    "df_merge.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9915, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:02:08.617937Z",
     "start_time": "2024-07-10T17:01:08.819981Z"
    }
   },
   "outputs": [],
   "source": [
    "# percorso dei file CSV\n",
    "csv_file_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/df_completo_GKV_13kmer.csv'\n",
    "cladeL_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/GK/img'\n",
    "cladeV_path = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/V/img'\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Funzione per caricare le immagini e convertirle in array\n",
    "def load_images(image_names, labels, cladeL_path, cladeV_path):\n",
    "    images = []\n",
    "    for image_name, label in zip(image_names, labels):\n",
    "        if label == 0:\n",
    "            image_path = os.path.join(cladeL_path, image_name)\n",
    "        else:\n",
    "            image_path = os.path.join(cladeV_path, image_name)\n",
    "        image = load_img(image_path, color_mode= \"grayscale\", target_size=(8192, 8192)) #Aggiunto color_mode per caricare l'immagine in scala di grigi\n",
    "        image_array = img_to_array(image)\n",
    "        images.append(image_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Carica le immagini\n",
    "X = load_images(df['Image_Name'], df['Label'], cladeL_path, cladeV_path)\n",
    "y = df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:02:08.649369Z",
     "start_time": "2024-07-10T17:02:08.622429Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:02:08.679886Z",
     "start_time": "2024-07-10T17:02:08.652359Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_targets(y):\n",
    "    \n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(y)\n",
    "    #print(integer_encoded)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    #print(onehot_encoded)\n",
    "    #y_train_enc = onehot_encoded.transform(y_train)\n",
    "    #y_test_enc = onehot_encoded.transform(y_test)\n",
    "    return onehot_encoded\n",
    "\n",
    "Y = prepare_targets(y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:02:14.147920Z",
     "start_time": "2024-07-10T17:02:08.682383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dividere il dataset in train e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels = np.argmax(y_test, axis=1) #[1,0] con la classe 0 mentre [0,1] con la classe 1\n",
    "\n",
    "# Trova le etichette uniche\n",
    "unique_labels = np.unique(y_test_labels)\n",
    "print(\"Etichette uniche:\", unique_labels)\n",
    "\n",
    "# Conta le occorrenze delle etichette\n",
    "counts = np.bincount(y_test_labels)\n",
    "print(\"Frequenze delle etichette:\", dict(zip(unique_labels, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividere il dataset in train e val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_val))\n",
    "print(np.shape(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:02:14.163914Z",
     "start_time": "2024-07-10T17:02:14.148918Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def get_CNN_model():\n",
    "    latent_dim = 100\n",
    "    hidden_activation = \"relu\"\n",
    "    kmer = 13  \n",
    "    n_chromosomes = 1  #Cambiato da 3 a 1 poiché l'immagine è aperta in scala di grigi\n",
    "\n",
    "   \n",
    "    rows = 8192\n",
    "    cols = 8192\n",
    "\n",
    "   \n",
    "    level = 3\n",
    "    kernel_size = 2 ** level\n",
    "    stride = 2 ** level\n",
    "\n",
    "    input_shape = (rows, cols, n_chromosomes)\n",
    "    \n",
    "    \n",
    "    n_classes = 2\n",
    "\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\", input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters=4, kernel_size=kernel_size, strides=stride, activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2 ** level, 2 ** level), strides=stride, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T17:02:24.535661Z",
     "start_time": "2024-07-10T17:02:14.166028Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "import os\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Parametri di addestramento\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "output_dir = '/media/musimathicslab/Seagate Basic/fcgr_snmg/13Kmer/Model_GKV'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Percorsi per salvare i pesi e il modello\n",
    "model_weights_path = os.path.join(output_dir, 'best_model_weights3.h5')\n",
    "model_path = os.path.join(output_dir, 'trained_model3.h5')\n",
    "\n",
    "# Callback per il checkpoint dei pesi e il logger CSV e early stopping\n",
    "model_checkpoint = ModelCheckpoint(model_weights_path, monitor='val_accuracy', save_best_only=True, save_weights_only=True, mode='max', verbose=1)\n",
    "csv_logger = CSVLogger(os.path.join(output_dir, 'training.log'))\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Definisci il modello\n",
    "CNN = get_CNN_model()\n",
    "\n",
    "# Addestramento del modello\n",
    "history = CNN.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val), callbacks=[model_checkpoint, csv_logger, early_stopping])\n",
    "\n",
    "# Valutazione del modello\n",
    "loss, accuracy = CNN.evaluate(X_test, y_test)\n",
    "\n",
    "# Salva l'intero modello\n",
    "CNN.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Predizione sui dati di test\n",
    "y_pred = CNN.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calcolo delle metriche\n",
    "classification_rep = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "\n",
    "# Print metrics to console\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score (Weighted)': f1,\n",
    "    'Precision (Weighted)': precision,\n",
    "    'Recall (Weighted)': recall,\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the metrics dictionary\n",
    "metrics_df = pd.DataFrame(list(metrics_dict.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Define the path to save the metrics\n",
    "metrics_file_path = os.path.join(output_dir, 'metrics.csv')\n",
    "\n",
    "# Save the metrics to the CSV file\n",
    "metrics_df.to_csv(metrics_file_path, index=False)\n",
    "\n",
    "# Save the classification report (as a CSV or plain text)\n",
    "classification_report_path = os.path.join(output_dir, 'classification_report.csv')\n",
    "classification_rep_df = pd.DataFrame(classification_rep).transpose()  # Convert classification report to DataFrame\n",
    "classification_rep_df.to_csv(classification_report_path)\n",
    "\n",
    "# Save the confusion matrix to a CSV file\n",
    "confusion_matrix_path = os.path.join(output_dir, 'confusion_matrix.csv')\n",
    "pd.DataFrame(conf_matrix).to_csv(confusion_matrix_path, index=False, header=False)\n",
    "\n",
    "\n",
    "# Carica la cronologia dell'addestramento dal logger CSV\n",
    "history_path = os.path.join(output_dir, 'training.log')\n",
    "history = pd.read_csv(history_path)\n",
    "\n",
    "# Plot della curva di loss (solo training)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['epoch'], history['loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot della curva di accuratezza (solo training)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['epoch'], history['accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Salva la figura come file immagine\n",
    "plot_file_path = os.path.join(output_dir, 'training_curves_GKV.png')\n",
    "plt.savefig(plot_file_path, dpi=300)  # Save the figure as a PNG file with high resolution\n",
    "plt.show()\n",
    "# Close the plot to avoid memory issues\n",
    "plt.close()\n",
    "\n",
    "print(f\"Training curves saved as {plot_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
